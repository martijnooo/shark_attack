{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_excel(r\"C:\\Users\\Martijn\\Downloads\\GSAF5.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goal: Commit the perfect shark murder\n",
    "# Problem Statement: Identify the correct location, activity, time of the day and what time of the year\n",
    "\n",
    "# Hypothesis 1: The perfect location is Australia (can be narrowed down on state level if fitting to the country)\n",
    "# Hypothesis 2: The perfect time of the year is early in the year on a morning\n",
    "# Hypothesis 3: The deadliest sharks will be white, tiger and bull\n",
    "\n",
    "# Columns we need: \"Fatal Y/N\", \"Country\", \"Date\", \"Activity\",  \"Time\"\n",
    "# -> GFM: Fatal & Country\n",
    "# -> LB: Date\n",
    "# -> MB: Activty & Time\n",
    "\n",
    "\n",
    "# further stuf:\n",
    "# - age, gender of the person we try to kill\n",
    "# species for \"e.g should be killed by white shark\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()\n",
    "# -> No duplicates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activity cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_time_activity_null = df[[\"Time\", \"Activity\"]].isnull().mean()\n",
    "activity_null = df_time_activity_null[\"Activity\"]\n",
    "print(f\"Activity has {round(activity_null,2)*100}% empty values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean column from any typing mistakes\n",
    "df.Activity = df.Activity.apply(lambda x: ' '.join(x.lower().split()).replace(\" \", \"_\").replace(\"-\",\"_\") if isinstance(x,str) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarise surfing activities into one\n",
    "df.Activity = df.Activity.replace({\"stand_up_paddleboarding\":\"surface_sport\", \"paddle_boarding\":\"surface_sport\", \"kite_surfing\": \"surface_sport\",\"windsurfing\":\"surface_sport\", \"surf_skiing\":\"surface_sport\",\"kayaking\":\"surface_sport\",\"rowing\":\"surface_sport\",\"canoeing\":\"surface_sport\", \"surfing\":\"surface_sport\", \"body_boarding\": \"surface_sport\", \"boogie_boarding\": \"surface_sport\"})\n",
    "#df['Activity'] = df['Activity'].str.replace(pat=r\"(?i)(\\S*surf\\S*|\\S*board\\S*)\", repl=\"surfing\", regex=True)\n",
    "# summarise diving activities\n",
    "df['Activity'] = df['Activity'].str.replace(pat=r\"(?i)(\\S*dive\\S*|\\S*divi\\S*)\", repl=\"diving_activities\", regex=True).replace({\"diving\":\"diving_activities\", \"snorkeling\":\"diving_activities\"})\n",
    "# summarise bathing\n",
    "df.Activity = df.Activity.replace({\"walking\":\"swimming\",\"wading\":\"swimming\",\"playing\":\"swimming\",\"floating_on_his_back\": \"swimming\", \"treading_water\": \"swimming\", \"body_surfing\":\"swimming\", \"bathing\":\"swimming\", \"standing\":\"swimming\", \"floating\":\"swimming\"})\n",
    "# summarise fishing\n",
    "df['Activity'] = df['Activity'].str.replace(pat=r\"(?i)(\\S*fish\\S*)\", repl=\"fishing\", regex=True)\n",
    "# disaster\n",
    "df.Activity = df.Activity.replace({\"fell_overboard\": \"accident\", \"sea_disaster\":\"accident\"})\n",
    "# categorize other activities\n",
    "df.Activity = df.Activity.apply(lambda x: x if x in [\"surface_sport\", \"diving_activities\", \"swimming\", \"fishing\", \"accident\"] else \"unqualified\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Activity.value_counts().nlargest(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_null = df_time_activity_null[\"Time\"]\n",
    "print(f\"Time has {round(time_null,2)*100}% empty values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Time'] = df['Time'].replace(to_replace=r'(?i).*noon.*', value='Afternoon', regex=True)\n",
    "df['Time'] = df['Time'].replace(to_replace=r'(?i).*morning.*', value='Morning', regex=True)\n",
    "df['Time'] = df['Time'].replace(to_replace=r'(?i).*Night.*', value='Night', regex=True)\n",
    "df.Time = df.Time.replace({\"P.M.\":\"Afternoon\", \"A.M.\":\"Morning\", \"Dusk\":\"Morning\", \"Midday\": \"Afternoon\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_time(time):\n",
    "    if pd.isna(time) or time == \"?\":\n",
    "        return \"Unknown\"\n",
    "    try:\n",
    "        hour = int(time[:2])\n",
    "        if 6 <= hour < 12:\n",
    "            return \"Morning\"\n",
    "        elif 12 <= hour < 18:\n",
    "            return \"Afternoon\"\n",
    "        elif 18 <= hour < 22:\n",
    "            return \"Evening\"\n",
    "        else:\n",
    "            return \"Night\"\n",
    "    except:\n",
    "        return time\n",
    "\n",
    "# Create a new column for time category based on time\n",
    "df['Time_Category'] = df['Time'].apply(categorize_time)\n",
    "df['Time_Category'].value_counts().nlargest(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "df_crosstab = pd.crosstab(df[\"Fatal Y/N\"], df.Activity)\n",
    "chi2_statistic, chi2_p_value, _, _ = chi2_contingency(df_crosstab)\n",
    "\n",
    "chi2_statistic, chi2_p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_date_column(df, column_name, date_format='%Y-%m-%d', fill_na=None):\n",
    "    \"\"\"\n",
    "    Cleans a date column in a DataFrame and returns both cleaned and invalid rows.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The DataFrame containing the date column.\n",
    "    - column_name (str): The name of the date column to clean.\n",
    "    - date_format (str, optional): The format to standardize the date to (default: '%Y-%m-%d').\n",
    "    - fill_na (str or pd.Timestamp, optional): A default date to fill missing/invalid values. If None, does not fill.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The DataFrame with the cleaned date column.\n",
    "    - pd.DataFrame: A DataFrame containing rows that had invalid dates.\n",
    "    - dict: A summary of how many rows were affected.\n",
    "    \"\"\"\n",
    "    df = df.copy()  # Avoid modifying the original DataFrame\n",
    "\n",
    "    # Convert column to datetime, coercing errors to NaT\n",
    "    df[column_name] = pd.to_datetime(df[column_name], errors='coerce')\n",
    "\n",
    "    # Separate valid and invalid date rows\n",
    "    valid_df = df[df[column_name].notna()].copy()\n",
    "    invalid_df = df[df[column_name].isna()].copy()\n",
    "\n",
    "    # Count invalid dates\n",
    "    invalid_count = len(invalid_df)\n",
    "\n",
    "    # Handle missing values in valid_df\n",
    "    if fill_na is not None:\n",
    "        valid_df[column_name] = valid_df[column_name].fillna(pd.Timestamp(fill_na))\n",
    "\n",
    "    # Standardize date format\n",
    "    valid_df[column_name] = valid_df[column_name].dt.strftime(date_format)\n",
    "\n",
    "    # Summary of changes\n",
    "    summary = {\n",
    "        \"total_rows\": len(df),\n",
    "        \"valid_rows\": len(valid_df),\n",
    "        \"invalid_rows\": invalid_count,\n",
    "        \"filled_rows\": invalid_count if fill_na else 0\n",
    "    }\n",
    "\n",
    "    print(\"Date Cleaning Summary:\", summary)\n",
    "    \n",
    "    return valid_df, invalid_df, summary\n",
    "\n",
    "cleaned_df, invalid_df, summary = clean_date_column(df, 'Date')\n",
    "\n",
    "print(\"\\nCleaned DataFrame:\")\n",
    "display(cleaned_df)\n",
    "\n",
    "print(\"\\nInvalid DataFrame:\")\n",
    "display(invalid_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datasci_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
