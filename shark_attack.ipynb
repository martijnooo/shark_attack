{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"GSAF5.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goal: Commit the perfect shark murder\n",
    "# Problem Statement: Identify the correct location, activity, time of the day and what time of the year\n",
    "\n",
    "# Hypothesis 1: The perfect location is Australia (can be narrowed down on state level if fitting to the country)\n",
    "# Hypothesis 2: The perfect time of the year is early in the year on a morning\n",
    "# Hypothesis 3: The deadliest sharks will be white, tiger and bull\n",
    "\n",
    "# Columns we need: \"Fatal Y/N\", \"Country\", \"Date\", \"Activity\",  \"Time\"\n",
    "# -> GFM: Fatal & Country\n",
    "# -> LB: Date\n",
    "# -> MB: Activty & Time\n",
    "\n",
    "\n",
    "# further stuf:\n",
    "# - age, gender of the person we try to kill\n",
    "# species for \"e.g should be killed by white shark\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()\n",
    "# -> No duplicates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activity cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goal: Commit the perfect shark murder\n",
    "# Problem Statement: Identify the correct location, activity,  time of the day and what time of the year\n",
    "\n",
    "df_time_activity_null = df[[\"Time\", \"Activity\"]].isnull().mean()\n",
    "activity_null = df_time_activity_null[\"Activity\"]\n",
    "print(f\"Activity has {round(activity_null,2)*100}% empty values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean column from any typing mistakes\n",
    "df.Activity = df.Activity.apply(lambda x: ' '.join(x.lower().split()).replace(\" \", \"_\").replace(\"-\",\"_\") if isinstance(x,str) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarise surfing activities into one\n",
    "df.Activity = df.Activity.replace({\"stand_up_paddleboarding\":\"surface_sport\", \"paddle_boarding\":\"surface_sport\", \"kite_surfing\": \"surface_sport\",\"windsurfing\":\"surface_sport\", \"surf_skiing\":\"surface_sport\",\"kayaking\":\"surface_sport\",\"rowing\":\"surface_sport\",\"canoeing\":\"surface_sport\", \"surfing\":\"surface_sport\", \"body_boarding\": \"surface_sport\", \"boogie_boarding\": \"surface_sport\"})\n",
    "#df['Activity'] = df['Activity'].str.replace(pat=r\"(?i)(\\S*surf\\S*|\\S*board\\S*)\", repl=\"surfing\", regex=True)\n",
    "# summarise diving activities\n",
    "df['Activity'] = df['Activity'].str.replace(pat=r\"(?i)(\\S*dive\\S*|\\S*divi\\S*)\", repl=\"diving_activities\", regex=True).replace({\"diving\":\"diving_activities\", \"snorkeling\":\"diving_activities\"})\n",
    "# summarise bathing\n",
    "df.Activity = df.Activity.replace({\"walking\":\"swimming\",\"wading\":\"swimming\",\"playing\":\"swimming\",\"floating_on_his_back\": \"swimming\", \"treading_water\": \"swimming\", \"body_surfing\":\"swimming\", \"bathing\":\"swimming\", \"standing\":\"swimming\", \"floating\":\"swimming\"})\n",
    "# summarise fishing\n",
    "df['Activity'] = df['Activity'].str.replace(pat=r\"(?i)(\\S*fish\\S*)\", repl=\"fishing\", regex=True)\n",
    "# disaster\n",
    "df.Activity = df.Activity.replace({\"fell_overboard\": \"accident\", \"sea_disaster\":\"accident\"})\n",
    "# categorize other activities\n",
    "df.Activity = df.Activity.apply(lambda x: x if x in [\"surface_sport\", \"diving_activities\", \"swimming\", \"fishing\", \"accident\"] else \"unqualified\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Activity.value_counts().nlargest(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_null = df_time_activity_null[\"Time\"]\n",
    "print(f\"Time has {round(time_null,2)*100}% empty values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Time'] = df['Time'].replace(to_replace=r'(?i).*noon.*', value='Afternoon', regex=True)\n",
    "df['Time'] = df['Time'].replace(to_replace=r'(?i).*morning.*', value='Morning', regex=True)\n",
    "df['Time'] = df['Time'].replace(to_replace=r'(?i).*Night.*', value='Night', regex=True)\n",
    "df.Time = df.Time.replace({\"P.M.\":\"Afternoon\", \"A.M.\":\"Morning\", \"Dusk\":\"Morning\", \"Midday\": \"Afternoon\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_time(time):\n",
    "    if pd.isna(time) or time == \"?\":\n",
    "        return \"Unknown\"\n",
    "    try:\n",
    "        hour = int(time[:2])\n",
    "        if 6 <= hour < 12:\n",
    "            return \"Morning\"\n",
    "        elif 12 <= hour < 18:\n",
    "            return \"Afternoon\"\n",
    "        elif 18 <= hour < 22:\n",
    "            return \"Evening\"\n",
    "        else:\n",
    "            return \"Night\"\n",
    "    except:\n",
    "        return time\n",
    "\n",
    "# Columns we need: \"Fatal Y/N\", \"Country\", \"Date\", \"Activity\",  \"Time\"\n",
    "# -> GFM: Fatal & Country\n",
    "# -> LB: Date\n",
    "# -> MB: Activty & Time\n",
    "\n",
    "\n",
    "# further stuf:\n",
    "# - age, gender of the person we try to kill\n",
    "# species for \"e.g should be killed by white shark\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "# Guillermo Working on Cleaning  Fatal Y/N\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to clean the 'Fatal Y/N' column in the DataFrame\n",
    "\n",
    "# Define a dictionary to map current values to 'Yes' or 'No'\n",
    "'''replacements = {\n",
    "    'Y': 'Yes',\n",
    "    'N': 'No',\n",
    "    'UNKNOWN': 'No',  # Example of treating unknown as 'No', adjust as needed\n",
    "    'nan': 'No',      # Handle missing values, adjust if they should be 'Yes' \n",
    "    # Add more mappings if necessary\n",
    "}\n",
    "'''\n",
    "# Sample data as a list\n",
    "values = ['N', 'Y', 'F', 'M', float('nan'), 'n', 'Nq', 'UNKNOWN', 2017, 'Y x 2', ' N', 'N ', 'y']\n",
    "\n",
    "# Define a dictionary for the mappings\n",
    "mapping = {\n",
    "    'N': 'N',\n",
    "    'Y': 'Y',\n",
    "    'F': 'U',           # Assuming 'F' and 'M' are unusual entries we'll consider unknown ('U')\n",
    "    'M': 'U',\n",
    "    'n': 'N',\n",
    "    'Nq': 'N',\n",
    "    'UNKNOWN': 'U',\n",
    "    2017: 'U',         # With the numerical could Keep numerical years unchanged or convert to 'U'\n",
    "    'Y x 2': 'Y',       # Considering 'Y x 2' as a form of 'Yes'\n",
    "    ' N': 'N',\n",
    "    'N ': 'N',\n",
    "    'y': 'Y'\n",
    "}\n",
    "\n",
    "# Apply the mapping to the column\n",
    "df['Fatal Y/N'] = df['Fatal Y/N'].replace(mapping).fillna('U')\n",
    "\n",
    "# Print the cleaned DataFrame column\n",
    "# print(df['Fatal Y/N'].value_counts())\n",
    "\n",
    "\n",
    "''' Sample data as a list\n",
    "values = ['N', 'Y', 'F', 'M', float('nan'), 'n', 'Nq', 'UNKNOWN', 2017, 'Y x 2', ' N', 'N ', 'y']\n",
    "\n",
    "different values in column fatal y/n:\n",
    " Fatal Y/N\n",
    "N          4877\n",
    "Y          1474\n",
    "UNKNOWN      71\n",
    " N            7\n",
    "F             5\n",
    "M             3\n",
    "n             1\n",
    "Nq            1\n",
    "2017          1\n",
    "Y x 2         1\n",
    "N             1\n",
    "y             1\n",
    "Name: count, dtype: int64\n",
    "['N' 'Y' 'F' 'M' nan 'n' 'Nq' 'UNKNOWN' 2017 'Y x 2' ' N' 'N ' 'y']\n",
    "Number of NaN values in 'Fatal Y/N' column: 561\n",
    "\n",
    "The result is:\n",
    "\n",
    "different values in column fatal y/n:\n",
    " Fatal Y/N\n",
    "N    4887\n",
    "Y    1476\n",
    "U     641\n",
    "Name: count, dtype: int64\n",
    "['N' 'Y' 'U']\n",
    "Number of NaN values in 'Fatal Y/N' column: 0'''\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Cleaning Country:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip leading/trailing whitespaces and convert to lowercase\n",
    "df['Country'] = df['Country'].str.lower().str.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NaN in the 'Country' column with 'unknown'\n",
    "df['Country'] = df['Country'].fillna('unknown')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st step: Clean the 'Country' column\n",
    "\n",
    "# Create the 'Region' column because too many entries with regions instead of countries in the 'Country' column. \n",
    "# that way we can still use the data even if it more generalized. Also there are a lot of small islands that have to be research to know where are they located.\n",
    "\n",
    "regions = [\n",
    "    'atlantic ocean', 'pacific ocean', 'indian ocean', 'red sea', 'mediterranean sea',\n",
    "    'caribbean sea', 'bay of bengal', 'gulf of aden', 'persian gulf', 'tasman sea',\n",
    "    'south china sea', 'north atlantic ocean', 'south atlantic ocean', 'north pacific ocean',\n",
    "    'south pacific ocean', 'southwest pacific ocean', 'central pacific', 'coral sea',\n",
    "    'mid atlantic ocean', 'mid-pacifc ocean', 'between portugal & india', 'coast of africa',\n",
    "    'the balkans', 'british isles', 'west indies', 'antarctica', 'ocean', 'africa',\n",
    "    'europe', 'north america', 'south america', 'oceania', 'middle east', 'asia'\n",
    "]\n",
    "# Clean 'Country' by removing '?' and whitespace\n",
    "df['Country_clean'] = df['Country'].str.replace('?', '', regex=False).str.strip()\n",
    "\n",
    "# Initialize 'Region' as 'unknown'\n",
    "df['Region'] = 'unknown'\n",
    "\n",
    "# Identify rows where cleaned 'Country' is a region\n",
    "is_region = df['Country_clean'].isin(regions)\n",
    "\n",
    "# Update 'Region' and 'Country' for these rows\n",
    "df.loc[is_region, 'Region'] = df.loc[is_region, 'Country_clean']\n",
    "df.loc[is_region, 'Country'] = 'unknown'\n",
    "\n",
    "# Drop the temporary 'Country_clean' column\n",
    "df.drop(columns=['Country_clean'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regroup regions and correct typos and more general regions\n",
    "region_grouping = {\n",
    "    # Ambiguous regions\n",
    "    'british isles': 'northern europe',\n",
    "    'ocean': 'unknown',\n",
    "    'coast of africa': 'africa',\n",
    "    'mid-pacifc ocean': 'pacific ocean',  # Corrected typo\n",
    "    'between portugal & india': 'unknown',\n",
    "    'the balkans': 'southeast europe',\n",
    "    'mid atlantic ocean': 'atlantic ocean',\n",
    "    'central pacific': 'pacific ocean',\n",
    "    \n",
    "    # Optional: Group smaller regions under continents\n",
    "    'caribbean sea': 'caribbean',\n",
    "    'bay of bengal': 'indian ocean',\n",
    "    'gulf of aden': 'indian ocean',\n",
    "    'persian gulf': 'middle east',\n",
    "    'tasman sea': 'pacific ocean',\n",
    "    'south china sea': 'pacific ocean',\n",
    "    'mediterranean sea': 'mediterranean',\n",
    "}\n",
    "# Step 1: Correct typos (e.g., \"mid-pacifc ocean\" → \"mid-pacific ocean\")\n",
    "df['Region'] = df['Region'].str.replace('pacifc', 'pacific')\n",
    "\n",
    "# Step 2: Apply grouping\n",
    "df['Region'] = df['Region'].replace(region_grouping)\n",
    "\n",
    "# Step 3: Handle remaining ambiguous entries (e.g., \"africa\", \"asia\")\n",
    "# Optional: If you want continents as regions, keep them. Otherwise:\n",
    "continent_to_region = {\n",
    "    'africa': 'africa',  # Keep if continents are valid regions for your analysis\n",
    "    'asia': 'asia',\n",
    "    'europe': 'europe',\n",
    "    'north america': 'north america',\n",
    "    'south america': 'south america',\n",
    "}\n",
    "df['Region'] = df['Region'].replace(continent_to_region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get current column list\n",
    "cols = df.columns.tolist()\n",
    "\n",
    "# Remove 'Region' from its current position\n",
    "cols.remove('Region')\n",
    "\n",
    "# Find the position of 'Country' and insert 'Region' before it\n",
    "country_position = cols.index('Country')\n",
    "cols.insert(country_position, 'Region')\n",
    "\n",
    "# Reorder the DataFrame\n",
    "df = df[cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "Cleaning more the Country column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2nd version of country mapping (after taking out the regions)\n",
    "country_mapping = {\n",
    "    # Fix typos and variations\n",
    "    'st kitts / nevis': 'saint kitts and nevis',\n",
    "    'st. martin': 'saint martin',\n",
    "    'st. maartin': 'saint martin',\n",
    "    'st helena, british overseas territory': 'saint helena',\n",
    "    'turks & caicos': 'turks and caicos',\n",
    "    'ceylon (sri lanka)': 'sri lanka',\n",
    "    'ceylon': 'sri lanka',\n",
    "    'british isles': 'united kingdom',  # Preserve as a distinct entity\n",
    "    'burma': 'myanmar',\n",
    "    'western samoa': 'samoa',\n",
    "    'british new guinea': 'papua new guinea',\n",
    "    'andaman / nicobar islandas': 'andaman and nicobar islands',\n",
    "    'andaman islands': 'andaman and nicobar islands',\n",
    "    'johnston island': 'johnston atoll',  # Preserve as a distinct entity\n",
    "    'british virgin islands': 'virgin islands',  # Preserve as a distinct entity\n",
    "    'netherlands antilles': 'netherlands antilles',  # Preserve as a distinct entity\n",
    "    'san domingo': 'dominican republic',\n",
    "    'grand cayman': 'cayman islands',  # Preserve as a distinct entity\n",
    "    'coast of africa': 'africa',  # Preserve as a distinct entity\n",
    "    'red sea / indian ocean': 'indian ocean',  # Preserve as a distinct entity\n",
    "    # 'between portugal & india': 'between portugal and india',  # Preserve as a distinct entity\n",
    "    # 'ocean': 'ocean',  # Preserve as a distinct entity\n",
    "    # 'unknown': 'unknown',\n",
    "\n",
    "    # Fix misspellings without grouping\n",
    "    'columbia': 'colombia',\n",
    "    \n",
    "    'united arab emirates (uae)': 'united arab emirates',\n",
    "    'solomon islands / vanuatu': 'solomon islands and vanuatu',\n",
    "    'trinidad & tobago': 'trinidad and tobago',\n",
    "                                                # Countries with double country \n",
    "    'egypt / israel': 'egypt',           # Egypt is often in total\n",
    "    'equatorial guinea / cameroon': 'equatorial guinea',\n",
    "    'iran / iraq': 'iran',\n",
    "    'italy / croatia': 'italy',\n",
    "    # 'red sea / indian ocean': 'red sea and indian ocean',\n",
    "    # 'mid-pacifc ocean': 'mid-pacific ocean',\n",
    "    # 'the balkans': 'the balkans',  # Preserve as a distinct entity\n",
    "}\n",
    "\n",
    "# Apply the mapping to the 'Country' column\n",
    "df['Country'] = df['Country'].replace(country_mapping)\n",
    "\n",
    "print(df['Country'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_mapping = {\n",
    "    # Australia and Oceania\n",
    "    'australia': 'australia and oceania',\n",
    "    'new caledonia': 'australia and oceania',\n",
    "    'new zealand': 'australia and oceania',\n",
    "    'fiji': 'australia and oceania',\n",
    "    'samoa': 'australia and oceania',\n",
    "    'papua new guinea': 'australia and oceania',\n",
    "    'solomon islands': 'australia and oceania',\n",
    "    'vanuatu': 'australia and oceania',\n",
    "    'kiribati': 'australia and oceania',\n",
    "    'tonga': 'australia and oceania',\n",
    "    'micronesia': 'australia and oceania',\n",
    "    'marshall islands': 'australia and oceania',\n",
    "    'palau': 'australia and oceania',\n",
    "    'cook islands': 'australia and oceania',\n",
    "    'french polynesia': 'australia and oceania',\n",
    "    'american samoa': 'australia and oceania',\n",
    "    'guam': 'australia and oceania',\n",
    "    'northern mariana islands': 'australia and oceania',\n",
    "    'new guinea': 'australia and oceania',\n",
    "    'admiralty islands': 'australia and oceania',\n",
    "    'johnston atoll': 'australia and oceania',\n",
    "\n",
    "    # Caribbean\n",
    "    'bahamas': 'caribbean',\n",
    "    'turks and caicos': 'caribbean',\n",
    "    'jamaica': 'caribbean',\n",
    "    'cuba': 'caribbean',\n",
    "    'dominican republic': 'caribbean',\n",
    "    'cayman islands': 'caribbean',\n",
    "    'aruba': 'caribbean',\n",
    "    'puerto rico': 'caribbean',\n",
    "    'saint kitts and nevis': 'caribbean',\n",
    "    'saint martin': 'caribbean',\n",
    "    'trinidad and tobago': 'caribbean',\n",
    "    'barbados': 'caribbean',\n",
    "    'grenada': 'caribbean',\n",
    "    'antigua': 'caribbean',\n",
    "    'virgin islands': 'caribbean',\n",
    "    'haiti': 'caribbean',\n",
    "    'belize': 'caribbean',\n",
    "    'netherlands antilles': 'caribbean',\n",
    "    'curacao': 'caribbean',\n",
    "\n",
    "    # North America\n",
    "    'usa': 'north america',\n",
    "    'canada': 'north america',\n",
    "    'mexico': 'north america',\n",
    "    'greenland': 'north america',\n",
    "    'bermuda': 'north america',\n",
    "\n",
    "    # South America\n",
    "    'brazil': 'south america',\n",
    "    'argentina': 'south america',\n",
    "    'colombia': 'south america',\n",
    "    'ecuador': 'south america',\n",
    "    'chile': 'south america',\n",
    "    'peru': 'south america',\n",
    "    'venezuela': 'south america',\n",
    "    'uruguay': 'south america',\n",
    "    'paraguay': 'south america',\n",
    "    'guyana': 'south america',\n",
    "    'falkland islands': 'south america',\n",
    "\n",
    "    # Europe\n",
    "    'spain': 'southern europe',\n",
    "    'portugal': 'southern europe',\n",
    "    'greece': 'southern europe',\n",
    "    'italy': 'southern europe',\n",
    "    'malta': 'southern europe',\n",
    "    'croatia': 'southern europe',\n",
    "    'montenegro': 'southern europe',\n",
    "    'slovenia': 'southern europe',\n",
    "    'france': 'western europe',\n",
    "    'united kingdom': 'western europe',\n",
    "    'ireland': 'western europe',\n",
    "    'iceland': 'northern europe',\n",
    "    'norway': 'northern europe',\n",
    "    'sweden': 'northern europe',\n",
    "    'azores': 'western europe',\n",
    "    'cyprus': 'southern europe',\n",
    "    'crete': 'southern europe',\n",
    "\n",
    "    # Africa\n",
    "    'egypt': 'northern africa',\n",
    "    'morocco': 'northern africa',\n",
    "    'libya': 'northern africa',\n",
    "    'tunisia': 'northern africa',\n",
    "    'algeria': 'northern africa',\n",
    "    'south africa': 'southern africa',\n",
    "    'mozambique': 'southern africa',\n",
    "    'madagascar': 'southern africa',\n",
    "    'mauritius': 'southern africa',\n",
    "    'seychelles': 'southern africa',\n",
    "    'comoros': 'southern africa',\n",
    "    'reunion': 'southern africa',\n",
    "    'reunion island': 'southern africa',\n",
    "    'saint helena': 'southern africa',\n",
    "    'angola': 'central africa',\n",
    "    'cameroon': 'central africa',\n",
    "    'gabon': 'central africa',\n",
    "    'equatorial guinea': 'central africa',\n",
    "    'kenya': 'eastern africa',\n",
    "    'tanzania': 'eastern africa',\n",
    "    'somalia': 'eastern africa',\n",
    "    'ethiopia': 'eastern africa',\n",
    "    'djibouti': 'eastern africa',\n",
    "    'senegal': 'western africa',\n",
    "    'nigeria': 'western africa',\n",
    "    'ghana': 'western africa',\n",
    "    'liberia': 'western africa',\n",
    "    'sierra leone': 'western africa',\n",
    "    'cape verde': 'western africa',\n",
    "    'namibia': 'southern africa',\n",
    "    'sudan': 'northern africa',\n",
    "    'sudan?': 'northern africa',\n",
    "\n",
    "    # Asia\n",
    "    'india': 'south asia',\n",
    "    'sri lanka': 'south asia',\n",
    "    'maldives': 'south asia',\n",
    "    'maldive islands': 'south asia',\n",
    "    'bangladesh': 'south asia',\n",
    "    'pakistan': 'south asia',\n",
    "    'iran': 'middle east',\n",
    "    'iraq': 'middle east',\n",
    "    'saudi arabia': 'middle east',\n",
    "    'yemen': 'middle east',\n",
    "    'jordan': 'middle east',\n",
    "    'lebanon': 'middle east',\n",
    "    'syria': 'middle east',\n",
    "    'kuwait': 'middle east',\n",
    "    'united arab emirates': 'middle east',\n",
    "    'qatar': 'middle east',\n",
    "    'oman': 'middle east',\n",
    "    'israel': 'middle east',\n",
    "    'palestinian territories': 'middle east',\n",
    "    'turkey': 'middle east',\n",
    "    'china': 'east asia',\n",
    "    'japan': 'east asia',\n",
    "    'south korea': 'east asia',\n",
    "    'north korea': 'east asia',\n",
    "    'taiwan': 'east asia',\n",
    "    'hong kong': 'east asia',\n",
    "    'thailand': 'southeast asia',\n",
    "    'malaysia': 'southeast asia',\n",
    "    'singapore': 'southeast asia',\n",
    "    'indonesia': 'southeast asia',\n",
    "    'philippines': 'southeast asia',\n",
    "    'vietnam': 'southeast asia',\n",
    "    'myanmar': 'southeast asia',\n",
    "    'cambodia': 'southeast asia',\n",
    "    'laos': 'southeast asia',\n",
    "    'brunei': 'southeast asia',\n",
    "\n",
    "    # Pacific\n",
    "    'hawaii': 'pacific',\n",
    "    'okinawa': 'pacific',\n",
    "    'new britain': 'pacific',\n",
    "    'java': 'pacific',\n",
    "    'solomon islands and vanuatu': 'pacific',\n",
    "    'federated states of micronesia': 'pacific',\n",
    "\n",
    "    # Other\n",
    "    'unknown': 'unknown',\n",
    "    'indian ocean': 'indian ocean',\n",
    "    'northern arabian sea': 'indian ocean',\n",
    "    'north sea': 'north sea',\n",
    "    'atlantic ocean': 'atlantic ocean',\n",
    "    'pacific ocean': 'pacific ocean',\n",
    "    'mediterranean sea': 'mediterranean',\n",
    "}\n",
    "\n",
    "# Apply the mapping to create/update the 'Region' column\n",
    "df['Region'] = df['Country'].map(region_mapping)\n",
    "\n",
    "# Fill any missing values with 'unknown'\n",
    "df['Region'] = df['Region'].fillna('unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Date\"] = df[\"Date\"].astype(str).str.replace(r\"\\s*00:00:00\", \"\", regex=True)\n",
    "# Define a regex pattern for valid dates\n",
    "valid_date_pattern = r\"^\\d{2}-[A-Za-z]{3}-\\d{4}$|^[A-Za-z]{3}-\\d{4}$\"\n",
    "\n",
    "# Create a mask to filter correct dates\n",
    "correct_dates = df[df[\"Date\"].str.match(valid_date_pattern, na=False)]\n",
    "\n",
    "# Store incorrect dates separately for cleaning\n",
    "incorrect_dates = df[~df[\"Date\"].str.match(valid_date_pattern, na=False)]\n",
    "# Define unwanted keywords\n",
    "unwanted_patterns = r\"(no date|world war|before|after|between|c[.\\s]?a[.\\s]?|circa|approx|around|B\\.?C\\.?|BCE|A\\.?D\\.?|CE|prior|\\bOR\\b)\"\n",
    "\n",
    "# Drop rows containing any of these words\n",
    "incorrect_dates = incorrect_dates[~incorrect_dates[\"Date\"].apply(lambda x: bool(re.search(unwanted_patterns, str(x), re.IGNORECASE)))]\n",
    "# Remove the words but keep the rest of the date\n",
    "incorrect_dates[\"Date\"] = incorrect_dates[\"Date\"].str.replace(r\"\\b(Reported|Late|Early)\\b\", \"\", case=False, regex=True).str.strip()\n",
    "# Define regex pattern for standalone years or decades\n",
    "standalone_year_pattern = r\"^\\d{4}s?$\"  \n",
    "\n",
    "# Drop rows that match exactly a 4-digit year or decade\n",
    "incorrect_dates = incorrect_dates[~incorrect_dates[\"Date\"].str.match(standalone_year_pattern, na=False)]\n",
    "\n",
    "# Fix missing hyphens in \"DD MMM YYYY\" and \"DD-MMM YYYY\" formats\n",
    "incorrect_dates[\"Date\"] = incorrect_dates[\"Date\"].str.replace(\n",
    "    r\"(\\d{1,2})\\s([A-Za-z]+)-?\\s?(\\d{4})\",  # Matches '22 Jul-2023', '5 Jul 2014', '3-Jul 2023'\n",
    "    r\"\\1-\\2-\\3\",  # Converts to '22-Jul-2023', '5-Jul-2014'\n",
    "    regex=True\n",
    ")\n",
    "incorrect_dates[\"Date\"] = incorrect_dates[\"Date\"].str.replace(\n",
    "    r\"(\\d{4})\\.(\\d{2})\\.(\\d{2})\", r\"\\3-\\2-\\1\", regex=True  # Rearranges YYYY.MM.DD → DD-MM-YYYY\n",
    ")\n",
    "incorrect_dates[\"Date\"] = incorrect_dates[\"Date\"].str.replace(\n",
    "    r\"(\\d{4}\\.\\d{2}\\.\\d{2})\\.\\w$\", r\"\\1\", regex=True  # Removes the trailing single letter\n",
    ")\n",
    "incorrect_dates[\"Date\"] = incorrect_dates[\"Date\"].str.replace(\n",
    "    r\"^[\\.\\s]+\", \"\", regex=True  # Removes leading dots and spaces\n",
    ")\n",
    "incorrect_dates[\"Date\"] = incorrect_dates[\"Date\"].str.replace(\n",
    "    r\"(\\d{1,2})-(Decp)(\\d{4})\", r\"\\1-Dec-\\3\", regex=True\n",
    ")\n",
    "# Fix missing hyphens in \"DD-MMM YYYY\" (e.g., \"08-Jun 2023\" → \"08-Jun-2023\")\n",
    "incorrect_dates[\"Date\"] = incorrect_dates[\"Date\"].str.replace(\n",
    "    r\"(\\d{1,2}-[A-Za-z]{3}) (\\d{4})\", r\"\\1-\\2\", regex=True\n",
    ")\n",
    "\n",
    "# Fix missing hyphens in \"DD MMM-YYYY\" (e.g., \"13-May 1871\" → \"13-May-1871\")\n",
    "incorrect_dates[\"Date\"] = incorrect_dates[\"Date\"].str.replace(\n",
    "    r\"(\\d{1,2}) ([A-Za-z]{3}-\\d{4})\", r\"\\1-\\2\", regex=True\n",
    ")\n",
    "# 1️⃣ Convert YYYY-MM-DD → DD-MMM-YYYY  (e.g., \"2025-02-22\" → \"22-Feb-2025\")\n",
    "incorrect_dates[\"Date\"] = incorrect_dates[\"Date\"].str.replace(\n",
    "    r\"(\\d{4})-(\\d{2})-(\\d{2})\", r\"\\3-\\2-\\1\", regex=True\n",
    ")\n",
    "\n",
    "# 2️⃣ Fix ordinal days (1st, 2nd, 3rd, etc.) → Normal numbers  (e.g., \"3rd-Oct-2016\" → \"03-Oct-2016\")\n",
    "incorrect_dates[\"Date\"] = incorrect_dates[\"Date\"].str.replace(\n",
    "    r\"(\\d{1,2})(st|nd|rd|th)-\", r\"\\1-\", regex=True\n",
    ")\n",
    "\n",
    "# 3️⃣ Standardize month names (e.g., \"31-July-2009\" → \"31-Jul-2009\")\n",
    "month_fix = {\n",
    "    \"January\": \"Jan\", \"February\": \"Feb\", \"March\": \"Mar\", \"April\": \"Apr\", \"May\": \"May\", \"June\": \"Jun\",\n",
    "    \"July\": \"Jul\", \"August\": \"Aug\", \"September\": \"Sep\", \"October\": \"Oct\", \"November\": \"Nov\", \"December\": \"Dec\"\n",
    "}\n",
    "for long_month, short_month in month_fix.items():\n",
    "    incorrect_dates[\"Date\"] = incorrect_dates[\"Date\"].str.replace(long_month, short_month, regex=True)\n",
    "\n",
    "# 4️⃣ Add leading zero to single-digit days (e.g., \"9-Mar-2018\" → \"09-Mar-2018\")\n",
    "incorrect_dates[\"Date\"] = incorrect_dates[\"Date\"].str.replace(\n",
    "    r\"\\b(\\d{1})-([A-Za-z]{3}-\\d{4})\", r\"0\\1-\\2\", regex=True\n",
    ")\n",
    "\n",
    "# 5️⃣ Fix incorrect \"Month-Day-Year\" order → Correct \"Day-Month-Year\" (e.g., \"May-17-1803\" → \"17-May-1803\")\n",
    "incorrect_dates[\"Date\"] = incorrect_dates[\"Date\"].str.replace(\n",
    "    r\"([A-Za-z]{3})-(\\d{1,2})-(\\d{4})\", r\"\\2-\\1-\\3\", regex=True\n",
    ")\n",
    "def convert_numeric_to_text_date(date_str):\n",
    "    \"\"\" Convert DD-MM-YYYY → DD-MMM-YYYY \"\"\"\n",
    "    try:\n",
    "        return datetime.datetime.strptime(date_str, \"%d-%m-%Y\").strftime(\"%d-%b-%Y\")\n",
    "    except ValueError:\n",
    "        return date_str  # If it can't be converted, return original\n",
    "\n",
    "# Apply the function to the Date column\n",
    "incorrect_dates[\"Date\"] = incorrect_dates[\"Date\"].apply(convert_numeric_to_text_date)\n",
    "# Reuse the same regex pattern to filter valid dates\n",
    "new_correct_dates = incorrect_dates[incorrect_dates[\"Date\"].str.match(valid_date_pattern, na=False)]\n",
    "\n",
    "# Store newly cleaned correct dates\n",
    "correct_dates = pd.concat([correct_dates, new_correct_dates])\n",
    "\n",
    "# Keep only still-incorrect dates for further cleaning\n",
    "incorrect_dates = incorrect_dates[~incorrect_dates[\"Date\"].str.match(valid_date_pattern, na=False)]\n",
    "# Convert 'Date' column to datetime for sorting, ignore errors to keep text-based dates\n",
    "correct_dates[\"Date_Sort\"] = pd.to_datetime(correct_dates[\"Date\"], format=\"%d-%b-%Y\", errors=\"coerce\")\n",
    "\n",
    "# Sort by 'Date_Sort' while keeping original 'Date' column intact\n",
    "correct_dates = correct_dates.sort_values(by=\"Date_Sort\").drop(columns=[\"Date_Sort\"])\n",
    "\n",
    "# Reset index\n",
    "correct_dates = correct_dates.reset_index(drop=True)\n",
    "\n",
    "# Extract year, month, and day separately for sorting\n",
    "correct_dates[\"Year\"] = correct_dates[\"Date\"].str.extract(r\"(\\d{4})\").astype(float)\n",
    "correct_dates[\"Month\"] = correct_dates[\"Date\"].str.extract(r\"([A-Za-z]{3})\")\n",
    "correct_dates[\"Day\"] = correct_dates[\"Date\"].str.extract(r\"(^\\d{1,2})\").astype(float)\n",
    "\n",
    "# Convert month names to numbers for proper sorting\n",
    "month_map = {\"Jan\": 1, \"Feb\": 2, \"Mar\": 3, \"Apr\": 4, \"May\": 5, \"Jun\": 6, \n",
    "             \"Jul\": 7, \"Aug\": 8, \"Sep\": 9, \"Oct\": 10, \"Nov\": 11, \"Dec\": 12}\n",
    "correct_dates[\"Month\"] = correct_dates[\"Month\"].map(month_map)\n",
    "\n",
    "# Fill NaNs with extreme values to ensure correct sorting\n",
    "correct_dates[\"Day\"] = correct_dates[\"Day\"].fillna(0)  # No day → 0 (comes last within same month)\n",
    "correct_dates[\"Month\"] = correct_dates[\"Month\"].fillna(0)  # No month → 0 (comes last within same year)\n",
    "\n",
    "# Sort in descending order: first by Year, then Month, then Day\n",
    "correct_dates = correct_dates.sort_values(by=[\"Year\", \"Month\", \"Day\"], ascending=[False, False, False])\n",
    "\n",
    "# Drop helper columns\n",
    "correct_dates = correct_dates.drop(columns=[\"Year\", \"Month\", \"Day\"]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_dates.to_excel(\"SharkAttack_clean.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
